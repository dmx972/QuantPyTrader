name: QuantPyTrader CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly builds at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_performance_tests:
        description: 'Run performance benchmarks'
        required: false
        default: 'false'
        type: boolean
      run_extended_tests:
        description: 'Run extended test suite'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.11.13'
  COVERAGE_MIN_PERCENTAGE: 80
  
jobs:
  # Code quality and linting
  code_quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy bandit safety
        pip install -r requirements.txt
    
    - name: Code formatting check (Black)
      run: |
        black --check --diff --color .
    
    - name: Import sorting check (isort)
      run: |
        isort --check-only --diff --color .
    
    - name: Linting (flake8)
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type checking (mypy)
      run: |
        mypy backtesting/ --ignore-missing-imports --no-strict-optional
      continue-on-error: true  # Type hints are not complete yet
    
    - name: Security check (bandit)
      run: |
        bandit -r backtesting/ -f json -o bandit-report.json
        bandit -r backtesting/ --severity-level medium
      continue-on-error: true
    
    - name: Dependency security check (safety)
      run: |
        safety check --json --output safety-report.json
        safety check
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Unit and integration tests
  test_suite:
    name: Test Suite (Python ${{ matrix.python-version }}, ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ['3.11']
        include:
          # Test additional Python versions on Linux only
          - os: ubuntu-latest
            python-version: '3.10'
          - os: ubuntu-latest
            python-version: '3.12'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install system dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-xdist pytest-timeout coverage[toml]
        pip install -r requirements.txt
    
    - name: Create test environment
      run: |
        python -c "import sys; print(f'Python {sys.version}')"
        python -c "import sqlite3; print(f'SQLite {sqlite3.sqlite_version}')"
    
    - name: Run unit tests
      run: |
        pytest tests/ -v --tb=short --timeout=300 \
               --cov=backtesting \
               --cov-report=xml \
               --cov-report=html \
               --cov-report=term-missing \
               --cov-fail-under=${{ env.COVERAGE_MIN_PERCENTAGE }} \
               -x tests/test_ukf_base.py \
               -x tests/test_integration.py \
               -x tests/test_simple_integration.py \
               -x tests/test_state_persistence.py
    
    - name: Run system integration tests
      run: |
        pytest tests/test_simple_end_to_end.py -v --tb=short
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: |
          htmlcov/
          coverage.xml
          .coverage

  # Performance benchmarking (optional/nightly)
  performance_tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: github.event.inputs.run_performance_tests == 'true' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark
        pip install -r requirements.txt
    
    - name: Run performance benchmarks
      run: |
        pytest tests/test_performance_benchmarks.py -v --tb=short \
               --benchmark-only \
               --benchmark-json=benchmark-results.json
      continue-on-error: true
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results
        path: benchmark-results.json

  # Extended testing suite (optional)
  extended_tests:
    name: Extended Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: github.event.inputs.run_extended_tests == 'true' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-timeout
        pip install -r requirements.txt
    
    - name: Run end-to-end workflow tests
      run: |
        pytest tests/test_end_to_end_workflows.py -v --tb=short --timeout=600
      continue-on-error: true
    
    - name: Run data generator functionality tests
      run: |
        pytest tests/test_data_generator_functionality.py -v --tb=short
    
    - name: Generate comprehensive test report
      run: |
        python -c "
        import sys
        from datetime import datetime
        
        print('=== EXTENDED TEST SUITE REPORT ===')
        print(f'Timestamp: {datetime.now().isoformat()}')
        print(f'Python Version: {sys.version}')
        print('Status: Extended tests completed')
        print('==================================')
        "

  # Documentation and examples validation
  documentation:
    name: Documentation & Examples
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install sphinx sphinx-rtd-theme
      continue-on-error: true
    
    - name: Validate README examples
      run: |
        # Extract and validate Python code blocks from README
        python -c "
        import re
        
        # Read README
        try:
            with open('README.md', 'r') as f:
                readme = f.read()
            print('README.md found and readable')
        except:
            print('README.md not found or not readable')
            
        # Check for basic project information
        required_sections = ['# QuantPyTrader', 'installation', 'usage']
        for section in required_sections:
            if section.lower() in readme.lower():
                print(f'✓ Found section: {section}')
            else:
                print(f'⚠ Missing section: {section}')
        "
    
    - name: Validate code examples
      run: |
        # Test that basic imports work
        python -c "
        try:
            from backtesting.results.storage import ResultsStorage
            from backtesting.export import quick_export
            print('✓ Core imports successful')
        except ImportError as e:
            print(f'⚠ Import error: {e}')
        "
    
    - name: Check docstring coverage
      run: |
        python -c "
        import ast
        import os
        from pathlib import Path
        
        def check_docstrings(file_path):
            with open(file_path, 'r') as f:
                try:
                    tree = ast.parse(f.read())
                except:
                    return 0, 0
                    
            total = 0
            documented = 0
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                    if not node.name.startswith('_'):  # Skip private methods
                        total += 1
                        if ast.get_docstring(node):
                            documented += 1
            
            return documented, total
        
        total_documented = 0
        total_functions = 0
        
        for py_file in Path('backtesting').rglob('*.py'):
            if '__pycache__' not in str(py_file):
                doc, tot = check_docstrings(py_file)
                total_documented += doc
                total_functions += tot
        
        if total_functions > 0:
            coverage = (total_documented / total_functions) * 100
            print(f'Docstring coverage: {coverage:.1f}% ({total_documented}/{total_functions})')
        else:
            print('No functions found to check')
        "

  # Build and packaging validation
  build_validation:
    name: Build & Packaging
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build wheel setuptools
    
    - name: Validate package structure
      run: |
        python -c "
        from pathlib import Path
        import sys
        
        # Check for essential files
        essential_files = [
            'backtesting/__init__.py',
            'tests/test_utils.py',
            'requirements.txt'
        ]
        
        missing = []
        for file in essential_files:
            if not Path(file).exists():
                missing.append(file)
        
        if missing:
            print(f'Missing essential files: {missing}')
            sys.exit(1)
        else:
            print('✓ All essential files present')
        
        # Check Python package structure
        init_file = Path('backtesting/__init__.py')
        if init_file.exists():
            with open(init_file, 'r') as f:
                content = f.read()
                if '__version__' in content:
                    print('✓ Version information found')
                if '__all__' in content:
                    print('✓ Public API defined')
        "
    
    - name: Test package importability
      run: |
        python -c "
        import sys
        sys.path.insert(0, '.')
        
        try:
            import backtesting
            print(f'✓ Package import successful: {backtesting.__name__}')
            
            # Try importing key modules
            from backtesting.results.storage import ResultsStorage
            from backtesting.export import quick_export
            print('✓ Key module imports successful')
            
        except ImportError as e:
            print(f'✗ Import failed: {e}')
            sys.exit(1)
        "

  # Deployment readiness check (on main branch)
  deployment_readiness:
    name: Deployment Readiness
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.ref == 'refs/heads/main'
    needs: [code_quality, test_suite, documentation, build_validation]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run deployment readiness tests
      run: |
        python -c "
        from datetime import datetime
        import sys
        
        print('=== DEPLOYMENT READINESS CHECK ===')
        print(f'Timestamp: {datetime.now().isoformat()}')
        print(f'Branch: main')
        print(f'Python Version: {sys.version_info}')
        
        # Test critical functionality
        try:
            from backtesting.results.storage import ResultsStorage
            from backtesting.export import quick_export
            from tests.test_utils import create_sample_portfolio_history
            
            # Create a quick end-to-end test
            import tempfile
            from pathlib import Path
            from datetime import date
            
            temp_dir = tempfile.mkdtemp()
            db_path = Path(temp_dir) / 'deployment_test.db'
            storage = ResultsStorage(db_path)
            
            # Create test backtest
            backtest_id = storage.create_backtest_session(
                strategy_name='Deployment Test',
                strategy_type='BE_EMA_MMCUKF',
                backtest_name='Readiness Check',
                start_date=date(2024, 1, 1),
                end_date=date(2024, 1, 31)
            )
            
            # Test data generation and storage
            portfolio_history = create_sample_portfolio_history(
                date(2024, 1, 1), date(2024, 1, 31), 100000.0
            )
            
            results = {
                'portfolio_history': portfolio_history,
                'performance': {'total_return': 0.05, 'volatility': 0.15, 'sharpe_ratio': 1.2, 'max_drawdown': -0.03}
            }
            
            storage.store_backtest_results(backtest_id, results)
            
            # Test export
            export_path = quick_export(
                storage, backtest_id=backtest_id, 
                template='sharing', output_dir=Path(temp_dir)
            )
            
            if Path(export_path).exists():
                print('✓ End-to-end functionality test passed')
                print('✓ System ready for deployment')
            else:
                print('✗ Export test failed')
                sys.exit(1)
                
            # Cleanup
            import shutil
            shutil.rmtree(temp_dir)
            
        except Exception as e:
            print(f'✗ Deployment readiness test failed: {e}')
            sys.exit(1)
        
        print('===================================')
        "
    
    - name: Generate deployment summary
      run: |
        echo "## Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "✅ All quality gates passed" >> $GITHUB_STEP_SUMMARY
        echo "✅ Test suite completed successfully" >> $GITHUB_STEP_SUMMARY
        echo "✅ Documentation validated" >> $GITHUB_STEP_SUMMARY  
        echo "✅ Build validation passed" >> $GITHUB_STEP_SUMMARY
        echo "✅ End-to-end functionality confirmed" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Status: READY FOR DEPLOYMENT** 🚀" >> $GITHUB_STEP_SUMMARY

  # Notification and reporting
  notification:
    name: CI/CD Notification
    runs-on: ubuntu-latest
    if: always()
    needs: [code_quality, test_suite, build_validation]
    
    steps:
    - name: Generate CI/CD Report
      run: |
        echo "=== QuantPyTrader CI/CD Pipeline Report ===" 
        echo "Timestamp: $(date -u)"
        echo "Trigger: ${{ github.event_name }}"
        echo "Branch: ${{ github.ref_name }}"
        echo "Commit: ${{ github.sha }}"
        echo ""
        echo "Job Status Summary:"
        echo "- Code Quality: ${{ needs.code_quality.result }}"
        echo "- Test Suite: ${{ needs.test_suite.result }}"
        echo "- Build Validation: ${{ needs.build_validation.result }}"
        echo ""
        
        if [[ "${{ needs.code_quality.result }}" == "success" && "${{ needs.test_suite.result }}" == "success" && "${{ needs.build_validation.result }}" == "success" ]]; then
          echo "✅ Pipeline Status: SUCCESS"
          echo "All critical checks passed. System is stable."
        else
          echo "❌ Pipeline Status: FAILED"
          echo "One or more critical checks failed. Please review."
        fi
        echo "============================================"